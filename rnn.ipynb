{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import jovian\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# self write tools\n",
    "from tools import LABEL_ENCODER\n",
    "from models import RNN"
   ]
  },
  {
   "source": [
    "# Check GPU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda: GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"Using {}: {}\".format(device, torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "source": [
    "# 1. Global Variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "TRAIN_CSV = 'input/split/train.csv'\n",
    "VALID_CSV = 'input/split/valid.csv'\n"
   ]
  },
  {
   "source": [
    "# 2. Preprocess data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 Read data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remov_duplicates(input): \n",
    "    input = input.split(\" \") \n",
    "    for i in range(0, len(input)): \n",
    "        input[i] = \"\".join(input[i]) \n",
    "    UniqW = Counter(input) \n",
    "    s = \" \".join(UniqW.keys()) \n",
    "    return s\n",
    "\n",
    "#tokenization\n",
    "tok = spacy.load('en')\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]\n",
    "\n",
    "def encode_sentence(text, vocab2index, N=15):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length\n",
    "\n",
    "def preprocess_dataset(path):\n",
    "\n",
    "    # read csv\n",
    "    reviews = pd.read_csv(path)\n",
    "\n",
    "    # combine categories\n",
    "    # reviews['description'] = reviews['noisyTextDescription']\n",
    "    reviews['description'] = reviews['gender'] + ' ' + reviews['baseColour'] + ' ' + reviews['usage'] + ' ' + reviews['noisyTextDescription']\n",
    "    reviews = reviews[['id', 'description', 'category']]\n",
    "\n",
    "    # remove duplicat\n",
    "    reviews['description'] = reviews['description'].apply(lambda x: remov_duplicates(x))\n",
    "    reviews['description_length'] = reviews['description'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    #count number of occurences of each word\n",
    "    counts = Counter()\n",
    "    for index, row in reviews.iterrows():\n",
    "        counts.update(tokenize(row['description']))\n",
    "    \n",
    "    #deleting infrequent words\n",
    "    print(\"num_words before:\",len(counts.keys()))\n",
    "    for word in list(counts):\n",
    "        if counts[word] < 2:\n",
    "            del counts[word]\n",
    "    print(\"num_words after:\",len(counts.keys()))\n",
    "\n",
    "    #creating vocabulary\n",
    "    vocab2index = {\"\":0, \"UNK\":1}\n",
    "    words = [\"\", \"UNK\"]\n",
    "    for word in counts:\n",
    "        vocab2index[word] = len(words)\n",
    "        words.append(word)\n",
    "\n",
    "    reviews['encoded'] = reviews['description'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
    "\n",
    "    return reviews, words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num_words before: 6968\nnum_words after: 6614\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([24, 10, 21, ...,  2, 24, 21])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\n",
    "reviews, words = preprocess_dataset(\"input/train.csv\")\n",
    "\n",
    "X = list(reviews['encoded'])\n",
    "y = list(reviews['category'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "\n",
    "y_train, y_valid = LABEL_ENCODER.transform(y_train), LABEL_ENCODER.transform(y_valid)\n",
    "y_valid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    best_valid_loss = float('inf')\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "\n",
    "        # save best model\n",
    "        if val_loss < best_valid_loss:\n",
    "            best_valid_loss = val_loss\n",
    "            torch.save(model.state_dict(), './output/rnn-model.pt')\n",
    "\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.2f%%, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "\n",
    "    \n",
    "\n",
    "    return sum_loss/total, correct/total * 100, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocab_size, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss 2.197, val loss 1.926, val accuracy 48.06%, and val rmse 9.682\n",
      "train loss 1.106, val loss 0.962, val accuracy 75.38%, and val rmse 6.534\n",
      "train loss 0.760, val loss 0.709, val accuracy 81.62%, and val rmse 5.507\n",
      "train loss 0.586, val loss 0.584, val accuracy 84.95%, and val rmse 5.061\n",
      "train loss 0.482, val loss 0.529, val accuracy 86.06%, and val rmse 4.928\n",
      "train loss 0.416, val loss 0.493, val accuracy 87.01%, and val rmse 4.712\n",
      "train loss 0.364, val loss 0.477, val accuracy 87.56%, and val rmse 4.724\n",
      "train loss 0.315, val loss 0.456, val accuracy 87.63%, and val rmse 4.802\n",
      "train loss 0.279, val loss 0.447, val accuracy 88.33%, and val rmse 4.617\n",
      "train loss 0.242, val loss 0.438, val accuracy 88.16%, and val rmse 4.679\n",
      "train loss 0.219, val loss 0.436, val accuracy 88.63%, and val rmse 4.596\n",
      "train loss 0.198, val loss 0.444, val accuracy 88.63%, and val rmse 4.606\n",
      "train loss 0.179, val loss 0.434, val accuracy 88.74%, and val rmse 4.562\n",
      "train loss 0.162, val loss 0.439, val accuracy 88.83%, and val rmse 4.476\n",
      "train loss 0.145, val loss 0.448, val accuracy 88.97%, and val rmse 4.510\n",
      "train loss 0.134, val loss 0.445, val accuracy 89.04%, and val rmse 4.478\n",
      "train loss 0.122, val loss 0.444, val accuracy 89.07%, and val rmse 4.525\n",
      "train loss 0.107, val loss 0.453, val accuracy 89.07%, and val rmse 4.516\n",
      "train loss 0.102, val loss 0.455, val accuracy 89.14%, and val rmse 4.457\n",
      "train loss 0.088, val loss 0.459, val accuracy 88.93%, and val rmse 4.566\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=100, lr=0.001)"
   ]
  },
  {
   "source": [
    "# Analyze results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('output/rnn-model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}